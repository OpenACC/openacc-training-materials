{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenACC Loop Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of the lab is intended for C/C++ programmers. The Fortran version of this lab is available [here](../Fortran/README.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will receive a warning five minutes before the lab instance shuts down. Remember to save your work! If you are about to run out of time, please see the [Post-Lab](#Post-Lab-Summary) section for saving this lab to view offline later.\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's execute the cell below to display information about the GPUs running on the server. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgaccelinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Our goal for this lab is to use the OpenACC Loop clauses to opimize our Parallel Loops.\n",
    "  \n",
    "  \n",
    "  \n",
    "![development_cycle.png](../files/images/development_cycle.png)\n",
    "\n",
    "This is the OpenACC 3-Step development cycle.\n",
    "\n",
    "**Analyze** your code, and predict where potential parallelism can be uncovered. Use profiler to help understand what is happening in the code, and where parallelism may exist.\n",
    "\n",
    "**Parallelize** your code, starting with the most time consuming parts. Focus on maintaining correct results from your program.\n",
    "\n",
    "**Optimize** your code, focusing on maximizing performance. Performance may not increase all-at-once during early parallelization.\n",
    "\n",
    "We are currently tackling the **optimize** step. We will include the OpenACC loop clauses to optimize the execution of our parallel loop nests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run the Code\n",
    "\n",
    "In the previous labs, we have built up a working parallel code that can run on both a multicore CPU and a GPU. Let's run the code and note the performance, so that we can compare the runtime to any future optimizations we make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgcc -fast -ta=tesla -Minfo=accel -o laplace_baseline jacobi.c laplace2d.c && ./laplace_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Analyze the Code\n",
    "\n",
    "If you would like a refresher on the code files that we are working on, you may view both of them using the two links below.\n",
    "\n",
    "[jacobi.c](../../../../edit/module6/English/C/jacobi.c)  \n",
    "[laplace2d.c](../../../../edit/module6/English/C/laplace2d.c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Profile the Code\n",
    "\n",
    "If you would like to profile the code, you may select <a href=\"/vnc\" target=\"_blank\">this link.</a> When prompted for a password, type `openacc`. This will open a noVNC window, then you may use PGPROF to profile our laplace code. The executable will be found in the `/home/openacc/labs/module6/English/C` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## OpenACC Loop Directive\n",
    "\n",
    "The loop directive allows us to mark specific loops for parallelization. The loop directive also allows us to map specific optimizations/alterations to our loops using **loop clauses**. Not all loop clauses are used to optimize our code; some are also used to verify code correctness. A few examples of the loop directive are as follows:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop < loop clauses >\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < loop code >\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop < loop clauses >\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < loop code >\n",
    "}\n",
    "```\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop < loop clauses >\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop < loop clauses >\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Also, including loop optimizations does not always optimize the code. It is up to the programmer to decide which loop optimizations will work best for their loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Clause\n",
    "\n",
    "When using the `kernels` directive, the compiler will decide which loops are, and are not, parallelizable. However, the programmer can override this compiler decision by using the `independent` clause. The `independent` clause is a way for the programmer to guarantee to the compiler that a loop is **parallelizable**.\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop independent\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < Parallel Loop Code >\n",
    "}\n",
    "\n",
    "#pragma acc kernels\n",
    "{\n",
    "    for(int i = 0; i < N; i++)\n",
    "    {\n",
    "        < Parallel Loop Code >\n",
    "    }\n",
    "    \n",
    "    #pragma acc loop independent\n",
    "    for(int i = 0; i < N; i++)\n",
    "    {\n",
    "        < Parallel Loop Code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In the second example, we have two loops. The compiler will make a decision whether or not the first loop is parallelizable. In the second loop however, we have included the independent clause. This means that the compiler will trust the programmer, and assume that the second loop is parallelizable.\n",
    "\n",
    "When using the `parallel` directive, the `independent` clause is automatically implied on all `loop` directives contained within. This means that you do not need to use the `independent` clause when you are using the `parallel` directive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Clause\n",
    "\n",
    "The `auto` clause is more-or-less the complete opposite of the `independent` clause. When you are using the `parallel` directive, the compiler will trust anything that the programmer decides. This means that if the programmer believes that a loop is parallelizable, the compiler will trust the programmer. However, if you include the auto clause with your loops, the compiler will double check the loops, and decide whether or not to parallelize them.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop auto\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < Parallel Loop Code >\n",
    "}\n",
    "```\n",
    "\n",
    "The `independent` clause is a way for the programmer to assert to the compiler that a loop is parallelizable. The `auto` clause is a way for the programmer to tell the compiler to analyze the loop, and to determine whether or not it is parallelizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq Clause\n",
    "\n",
    "The `seq` clause (short for *\"sequential\"*) is used to define a loop that should run sequentially on the parallel hardware. This loop clause is usually automatically applied to large, multidimensional loop nests, since the compiler may only be able to describe parallelism for the outer-most loops. For example:\n",
    "\n",
    "```cpp\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < Loop Code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The compiler may only be able to parallelize the `i` and `j` loops, and will choose to run the `k` loop **sequentially**. The `seq` clause is also useful for running very small, nested loops sequentially. For example:\n",
    "\n",
    "```cpp\n",
    "for(int i = 0; i < 1000000; i++)\n",
    "{\n",
    "    for(int j = 0; j < 4; j++)\n",
    "    {\n",
    "        for(int k = 0; k < 1000000; k++)\n",
    "        {\n",
    "            < Loop Code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The middle loop is very small, and will most likely not benefit from parallelization. To fix this, we may apply the `seq` clause as follows:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop\n",
    "for(int i = 0; i < 1000000; i++)\n",
    "{\n",
    "    #pragma acc loop seq\n",
    "    for(int j = 0; j < 4; j++)\n",
    "    {\n",
    "        #pragma acc loop\n",
    "        for(int k = 0; k < 1000000; k++)\n",
    "        {\n",
    "            < Loop Code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this code snippet, the middle loop will be run sequentially, while the outer-most loop and inner-most loop will be run in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Clause\n",
    "\n",
    "Up to this point, we have technically been using the `reduction` clause in our laplace code. We were not explicitly defining the reduction, instead the compiler has been automatically applying the reduction clause to our code. Let's look at one of the loops from within our `laplace2d.c` code file.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop present(A,Anew)\n",
    "for( int j = 1; j < n-1; j++\n",
    "{\n",
    "    #pragma acc loop\n",
    "    for( int i = 1; i < m-1; i++ )\n",
    "    {\n",
    "        Anew[OFFSET(j, i, m)] = 0.25 * ( A[OFFSET(j, i+1, m)] + A[OFFSET(j, i-1, m)] \n",
    "                                       + A[OFFSET(j-1, i, m)] + A[OFFSET(j+1, i, m)] );\n",
    "        error = fmax( error, fabs(Anew[OFFSET(j, i, m)] - A[OFFSET(j, i , m)]));\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "More specifically, let's focus on this single line of code:\n",
    "\n",
    "```cpp\n",
    "error = fmax( error, fabs(Anew[OFFSET(j, i, m)] - A[OFFSET(j, i , m)]));\n",
    "```\n",
    "\n",
    "Each iteration of our inner-loop will write to the value `error`. When we are running thousands of these loop iterations **simultaneously**, it can become very dangerous to let all of them write directly to `error`. To fix this, we must use the OpenACC `reduction` clause. Let's look at the syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop reduction(operator:value)\n",
    "```\n",
    "\n",
    "And let's look at a quick example of the use.\n",
    "\n",
    "```ccp\n",
    "#pragma acc parallel loop reduction(+:sum)\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    sum += A[i];\n",
    "}\n",
    "```\n",
    "\n",
    "This is a list of all of the available operators in OpenACC.\n",
    "\n",
    "|Operator    |Example                     |Description           |\n",
    "|:----------:|:---------------------------|:---------------------|\n",
    "|+           |reduction(+:sum)            |Mathematical summation|\n",
    "|*           |reduction(*:product)        |Mathematical product  |\n",
    "|max         |reduction(max:maximum)      |Maximum value         |\n",
    "|min         |reduction(min:minimum)      |Minimum value         |\n",
    "|&           |reduction(&:val)            |Bitwise AND           |\n",
    "|&#124;      |reduction(&#124;:val)       |Bitwise OR            |\n",
    "|&&          |reduction(&&:bool)          |Logical AND           |\n",
    "|&#124;&#124;|reduction(&#124;&#124;:bool)|Logical OR            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Implementing the Reduction Clause\n",
    "\n",
    "We are compiling our code with the PGI compiler, which is automatically able to include the reduction clause. However, in other compilers, we may not be as fortunate. Use the following link to add the `reduction` clause with the `max` operator to our code.\n",
    "\n",
    "[laplace2d.c](../../../../edit/module6/English/C/laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "You may then run the following script to verify that the compiler is properly recognizing your reduction clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgcc -ta=tesla -Minfo=accel -o laplace_reduction jacobi.c laplace2d.c && ./laplace_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also check your answer by selecting the following link.\n",
    "\n",
    "[laplace2d.c](../../../../edit/module6/English/C/solutions/reduction/laplace2d.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Clause\n",
    "\n",
    "The private clause allows us to mark certain variables (and even arrays) as *\"private\"*. The best way to visualize it is with an example:\n",
    "\n",
    "```cpp\n",
    "int tmp;\n",
    "\n",
    "#pragma acc parallel loop private(tmp)\n",
    "for(int i = 0; i < N/2; i++)\n",
    "{\n",
    "    tmp = A[i];\n",
    "    A[i] = A[N-i-1];\n",
    "    A[N-i-1] = tmp;\n",
    "}\n",
    "```\n",
    "\n",
    "In this code, each thread will have its own *private copy of tmp*. You may also declare static arrays as private, like this:\n",
    "\n",
    "```cpp\n",
    "int tmp[10];\n",
    "\n",
    "#pragma acc parallel loop private(tmp[0:10])\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    < Loop code that uses the tmp array >\n",
    "}\n",
    "```\n",
    "\n",
    "When using *private variables*, the variable only exists within the private scope. This generally means that the private variable only exists for a single loop iteration, and the values you store in the private variable cannot extend out of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse Clause\n",
    "\n",
    "This is our first true *loop optimization*. The `collapse` clause allows us to transform a multi-dimensional loop nests into a single-dimensional loop. This process is helpful for increasing the overall length (which usually increases parallelism) of our loops, and will often help with memory locality. Let's look at the syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop collapse( N )\n",
    "```\n",
    "\n",
    "Where N is the number of loops to collapse.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop collapse( 3 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "This code will combine the 3-dimensional loop nest into a single 1-dimensional loop. It is important to note that when using the `collapse` clause, the inner loops should not have their own `loop` directive. What this means is that the following code snippet is **incorrect** and will give a warning when compiling.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop collapse( 3 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        #pragma acc loop\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Collapse Clause\n",
    "\n",
    "Use the following link to edit our code. Use the `collapse` clause to collapse our multi-dimensional loops into a single dimensional loop.\n",
    "\n",
    "[laplace2d.c](../../../../edit/module6/English/C/laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgcc -ta=tesla -Minfo=accel -o laplace_collapse jacobi.c laplace2d.c && ./laplace_collapse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile Clause\n",
    "\n",
    "The `tile` clause allows us to break up a multi-dimensional loop into *tiles*, or *blocks*. This is often useful for increasing memory locality in some codes. Let's look at the syntax.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( x, y, z, ... )\n",
    "```\n",
    "\n",
    "Our tiles can have as many dimensions as we want, though we must be careful to not create a tile that is too large. Let's look at an example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( 32, 32 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The above code will break our loop iterations up into 32x32 tiles (or blocks), and then execute those blocks in parallel. Let's look at a slightly more specific code.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( 32, 32 )\n",
    "for(int i = 0; i < 128; i++)\n",
    "{\n",
    "    for(int j = 0; j < 128; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this code, we have 128x128 loop iterations, which are being broken up into 32x32 tiles. This means that we will have 16 tiles, each tile being size 32x32. Similar to the `collapse` clause, the inner loops should not have the `loop` directive. This means that the following code is **incorrect** and will give a warning when compiling.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop tile( 32, 32 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Tile Clause\n",
    "\n",
    "Use the following link to edit our code. Replace the `collapse` clause with the `tile` clause to break our multi-dimensional loops into smaller tiles. Try using a variety of different tile sizes, but always keep one of the dimensions as a **multiple of 32**. We will talk later about why this is important.\n",
    "\n",
    "[laplace2d.c](../../../../edit/module6/English/C/laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgcc -ta=tesla -Minfo=accel -o laplace_tile jacobi.c laplace2d.c && ./laplace_tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gang/Worker/Vector\n",
    "\n",
    "This is our last optimization, and arguably the most important one. In OpenACC, the concepts of *Gang*, *Worker*, and *Vector* are used to define additional levels of parallelism. Specifically for NVIDIA GPUs, gang worker vector will define the *organization* of our GPU threads (or sometimes referred to as the *decomposition* of or loop iterations). Each loop will have an optimal Gang Worker Vector implementation, and finding that correct implementation will often take a bit of thinking, and possibly some trial and error. So let's explain how Gang Worker Vector actually works.\n",
    "\n",
    "![gang_worker_vector.png](../files/images/gang_worker_vector.png)\n",
    "\n",
    "This image represents a single **gang**. When parallelizing our **for loops**, the **loop iterations** will be **broken up evenly** among a number of gangs. Each gang will contain a number of **threads**. These threads are organized into **blocks**. A **worker** is a row of threads. In the above graphic, there are 3 **workers**, which means that there are 3 rows of threads. The **vector** refers to how long each row is. So in the above graphic, the vector is 8, because each row is 8 threads long.\n",
    "\n",
    "By default, when programming for a GPU, **gang** and **vector** paralleism is automatically applied. Let's see a simple GPU sample code where we explicitly show how the gang and vector works.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The outer loop will be evenly spread across a number of **gangs**. Then, within those gangs, the inner-loop will be executed in parallel across the **vector**. This is a process that usually happens automatically, however, we can usually achieve better performance by optimzing the gang worker vector ourselves.\n",
    "\n",
    "Lets look at an example where using gang worker vector can greatly increase a loops parallelism.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector\n",
    "    for(int j = 0; j < M; k++)\n",
    "    {\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this loop, we have **gang-level** parallelism on the outer-loop, and **vector-level** parallelism on the middle-loop. However, the inner-loop does not have any parallelism. This means that each thread will be running the inner-loop, however, GPU threads aren't really made to run entire loops. To fix this, we could use **worker-level** parallelism to add another layer.\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop worker\n",
    "    for(int j = 0; j < M; k++)\n",
    "    {\n",
    "        #pragma acc loop vector\n",
    "        for(int k = 0; k < Q; k++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Now, the outer-loop will be split across the gangs, the middle-loop will be split across the workers, and the inner loop will be executed by the threads within the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gang, Worker, Vector Syntax\n",
    "\n",
    "We have been showing really general examples of gang worker vector so far. One of the largest benefits of gang worker vector is the ability to explicitly define how many gangs and workers you need, and how many threads should be in the vector. Let's look at the syntax for the `parallel` directive:\n",
    "\n",
    "```cpp\n",
    "#pragma acc parallel num_gangs( 2 ) num_workers( 4 ) vector_length( 32 )\n",
    "{\n",
    "    #pragma acc loop gang worker\n",
    "    for(int i = 0; i < N; i++)\n",
    "    {\n",
    "        #pragma acc loop vector\n",
    "        for(int j = 0; j < M; j++)\n",
    "        {\n",
    "            < loop code >\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "And now the syntax for the `kernels` directive:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang( 2 ) worker( 4 )\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector( 32 )\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avoid Wasting Threads\n",
    "\n",
    "When parallelizing small arrays, you have to be careful that the number of threads within your vector is not larger than the number of loop iterations. Let's look at a simple example:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang\n",
    "for(int i = 0; i < 1000000000; i++)\n",
    "{\n",
    "    #pragma acc loop vector(256)\n",
    "    for(int j = 0; j < 32; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In this code, we are parallelizing an inner-loop that has 32 iterations. However, our vector is 256 threads long. This means that when we run this code, we will have a lot more threads than loop iterations, and a lot of the threads will be sitting idly. We could fix this in a few different ways, but let's use **worker-level parallelism** to fix it.\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang worker(8)\n",
    "for(int i = 0; i < 1000000000; i++)\n",
    "{\n",
    "    #pragma acc loop vector(32)\n",
    "    for(int j = 0; j < 32; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Originally we had 1 (implied) worker, that contained 256 threads. Now, we have 8 workers that each have only 32 threads. We have eliminated all of our wasted threads by reducing the length of the **vector** and increasing the number of **workers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Rule of 32 (Warps)\n",
    "\n",
    "The general rule of thumb for programming for NVIDIA GPUs is to always ensure that your vector length is a multiple of 32 (which means 32, 64, 96, 128, ... 512, ... 1024... etc.). This is because NVIDIA GPUs are optimized to use *warps*. Warps are groups of 32 threads that are executing the same computer instruction. So as a reference:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector(32)\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "will perform much better than:\n",
    "\n",
    "```cpp\n",
    "#pragma acc kernels loop gang\n",
    "for(int i = 0; i < N; i++)\n",
    "{\n",
    "    #pragma acc loop vector(31)\n",
    "    for(int j = 0; j < M; j++)\n",
    "    {\n",
    "        < loop code >\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Gang, Worker, and Vector Clauses\n",
    "\n",
    "Use the following link to edit our code. Replace our ealier clauses with **gang, worker, and vector** To reorganize our thread blocks. Try it using a few different numbers, but always keep the vector length as a **multiple of 32** to fully utilize **warps**.\n",
    "\n",
    "[laplace2d.c](../../../../edit/module6/English/C/laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgcc -ta=tesla -Minfo=accel -o laplace_gang_worker_vector jacobi.c laplace2d.c && ./laplace_gang_worker_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Everything we Learned\n",
    "\n",
    "Now that we have covered the various ways to edit our loops, apply this knowledge to our laplace code. Try mixing some of the loop clauses, and see how the loop optimizations will differ between the parallel and the kernels directive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may run the following script to reset your code with the `kernels` directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./solutions/base_parallel/kernels/laplace2d.c ./laplace2d.c && echo \"Reset Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may run the following script to reset your code with the `parallel` directive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./solutions/base_parallel/parallel/laplace2d.c ./laplace2d.c && echo \"Reset Finished\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the following link to edit our laplace code.\n",
    "\n",
    "[laplace2d.c](../../../../edit/module6/English/C/laplace2d.c)  \n",
    "(make sure to save your code with ctrl+s)\n",
    "\n",
    "Then run the following script to see how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pgcc -ta=tesla -Minfo=accel -o laplace jacobi.c laplace2d.c && ./laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Our primary goal when using OpenACC is to parallelize our large for loops. To accomplish this, we must use the OpenACC `loop` directive and loop clauses. There are many ways to alter and optimize our loops, though it is up to the programmer to decide which route is the best to take. At this point in the lab series, you should be able to begin parallelizing your own personal code, and to be able to achieve a relatively high performance using OpenACC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Task\n",
    "\n",
    "If you would like some additional lessons on using OpenACC, there is an Introduction to OpenACC video series available from the OpenACC YouTube page. If you haven't already, I recommend watching this 6 part series. Each video is under 10 minutes, and will give a visual, and hands-on look at a lot of the material we have covered in these labs. The following link will bring you to Part 1 of the series.\n",
    "\n",
    "[Introduction to Parallel Programming with OpenACC - Part 1](https://youtu.be/PxmvTsrCTZg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommend you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well.\n",
    "\n",
    "You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -f openacc_files.zip\n",
    "zip -r openacc_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download the zip file [here](files/openacc_files.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Licensing\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
